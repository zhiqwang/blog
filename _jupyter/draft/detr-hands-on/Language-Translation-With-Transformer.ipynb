{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE TRANSLATION WITH TRANSFORMER\n",
    "\n",
    "This tutorial shows, how to train a translation model from scratch using Transformer. We will be using Multi30k dataset to train a French to English translation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext has utilities for creating datasets that can be easily iterated through for the purposes of creating a language translation model. In this example, we show how to tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor.\n",
    "\n",
    "To run this tutorial, first install spacy using pip or conda. Next, download the raw data for the English and French Spacy tokenizers from https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5e743e1cc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import extract_archive\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_helper import (\n",
    "    Seq2SeqTransformer,\n",
    "    build_vocab,\n",
    "    data_process,\n",
    "    generate_batch,\n",
    "    train_epoch,\n",
    "    evaluate,\n",
    "    translate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data-bin')\n",
    "\n",
    "train_urls = ('train.fr', 'train.en')\n",
    "val_urls = ('val.fr', 'val.en')\n",
    "test_urls = ('test_2016_flickr.fr', 'test_2016_flickr.en')\n",
    "\n",
    "train_filepaths = [data_path / url for url in train_urls]\n",
    "val_filepaths = [data_path / url for url in val_urls]\n",
    "test_filepaths = [data_path / url for url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab = build_vocab(train_filepaths[0], fr_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(train_filepaths, fr_vocab, fr_tokenizer, en_vocab, en_tokenizer)\n",
    "val_data = data_process(val_filepaths, fr_vocab, fr_tokenizer, en_vocab, en_tokenizer)\n",
    "test_data = data_process(test_filepaths, fr_vocab, fr_tokenizer, en_vocab, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = fr_vocab['<pad>']\n",
    "BOS_IDX = fr_vocab['<bos>']\n",
    "EOS_IDX = fr_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "The last torch specific feature we’ll use is the DataLoader, which is easy to use since it takes the data as its first argument. Specifically, as the docs say: DataLoader combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.\n",
    "\n",
    "Please pay attention to `collate_fn` (optional) that merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = partial(generate_batch, start_symbol=BOS_IDX, end_symbol=EOS_IDX, padding_symbol=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    10   11\n",
       "0   0.0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       "1   0.0  0.0 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       "2   0.0  0.0  0.0 -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
       "3   0.0  0.0  0.0  0.0 -inf -inf -inf -inf -inf -inf -inf -inf\n",
       "4   0.0  0.0  0.0  0.0  0.0 -inf -inf -inf -inf -inf -inf -inf\n",
       "5   0.0  0.0  0.0  0.0  0.0  0.0 -inf -inf -inf -inf -inf -inf\n",
       "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0 -inf -inf -inf -inf -inf\n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -inf -inf -inf -inf\n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -inf -inf -inf\n",
       "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -inf -inf\n",
       "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -inf\n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformer_helper import create_mask\n",
    "\n",
    "src, tgt = next(iter(valid_iter))\n",
    "src = src.to(device)\n",
    "tgt = tgt.to(device)\n",
    "\n",
    "tgt_input = tgt[:-1, :]\n",
    "\n",
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "    src, tgt_input, PAD_IDX, device)\n",
    "\n",
    "pd.DataFrame(tgt_mask[0:12, 0:12].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer!\n",
    "\n",
    "Transformer is a Seq2Seq model introduced in [“Attention is all you need”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper for solving machine translation task. Transformer model consists of an encoder and decoder block each containing fixed number of layers.\n",
    "\n",
    "Encoder processes the input sequence by propogating it, through a series of Multi-head Attention and Feed forward network layers. The output from the Encoder referred to as `memory`, is fed to the decoder along with target tensors. Encoder and decoder are trained in an end-to-end fashion using teacher forcing technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters and instantiate model\n",
    "SRC_VOCAB_SIZE = len(fr_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    NHEAD,\n",
    "    EMB_SIZE,\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    FFN_HID_DIM,\n",
    ")\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Train loss: 5.295, Val loss: 3.985, Epoch time = 24.454s\n",
      "Epoch:  2, Train loss: 3.572, Val loss: 3.042, Epoch time = 24.331s\n",
      "Epoch:  3, Train loss: 2.843, Val loss: 2.560, Epoch time = 24.149s\n",
      "Epoch:  4, Train loss: 2.408, Val loss: 2.261, Epoch time = 24.321s\n",
      "Epoch:  5, Train loss: 2.103, Val loss: 2.061, Epoch time = 24.221s\n",
      "Epoch:  6, Train loss: 1.875, Val loss: 1.912, Epoch time = 24.330s\n",
      "Epoch:  7, Train loss: 1.694, Val loss: 1.823, Epoch time = 24.263s\n",
      "Epoch:  8, Train loss: 1.548, Val loss: 1.756, Epoch time = 24.370s\n",
      "Epoch:  9, Train loss: 1.423, Val loss: 1.681, Epoch time = 24.223s\n",
      "Epoch: 10, Train loss: 1.319, Val loss: 1.639, Epoch time = 24.484s\n",
      "Epoch: 11, Train loss: 1.226, Val loss: 1.603, Epoch time = 24.360s\n",
      "Epoch: 12, Train loss: 1.142, Val loss: 1.555, Epoch time = 24.409s\n",
      "Epoch: 13, Train loss: 1.068, Val loss: 1.527, Epoch time = 24.250s\n",
      "Epoch: 14, Train loss: 1.002, Val loss: 1.507, Epoch time = 24.285s\n",
      "Epoch: 15, Train loss: 0.939, Val loss: 1.509, Epoch time = 24.425s\n",
      "Epoch: 16, Train loss: 0.883, Val loss: 1.496, Epoch time = 24.337s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer, PAD_IDX, device)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter, PAD_IDX, device)\n",
    "    print((f\"Epoch: {epoch:2d}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "           f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following results during model training.\n",
    "\n",
    "```R\n",
    "Epoch:  1, Train loss: 5.295, Val loss: 3.985, Epoch time = 24.454s\n",
    "Epoch:  2, Train loss: 3.572, Val loss: 3.042, Epoch time = 24.331s\n",
    "Epoch:  3, Train loss: 2.843, Val loss: 2.560, Epoch time = 24.149s\n",
    "Epoch:  4, Train loss: 2.408, Val loss: 2.261, Epoch time = 24.321s\n",
    "Epoch:  5, Train loss: 2.103, Val loss: 2.061, Epoch time = 24.221s\n",
    "Epoch:  6, Train loss: 1.875, Val loss: 1.912, Epoch time = 24.330s\n",
    "Epoch:  7, Train loss: 1.694, Val loss: 1.823, Epoch time = 24.263s\n",
    "Epoch:  8, Train loss: 1.548, Val loss: 1.756, Epoch time = 24.370s\n",
    "Epoch:  9, Train loss: 1.423, Val loss: 1.681, Epoch time = 24.223s\n",
    "Epoch: 10, Train loss: 1.319, Val loss: 1.639, Epoch time = 24.484s\n",
    "Epoch: 11, Train loss: 1.226, Val loss: 1.603, Epoch time = 24.360s\n",
    "Epoch: 12, Train loss: 1.142, Val loss: 1.555, Epoch time = 24.409s\n",
    "Epoch: 13, Train loss: 1.068, Val loss: 1.527, Epoch time = 24.250s\n",
    "Epoch: 14, Train loss: 1.002, Val loss: 1.507, Epoch time = 24.285s\n",
    "Epoch: 15, Train loss: 0.939, Val loss: 1.509, Epoch time = 24.425s\n",
    "Epoch: 16, Train loss: 0.883, Val loss: 1.496, Epoch time = 24.337s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: `A group of people standing outside an igloo .`.\n"
     ]
    }
   ],
   "source": [
    "src_language = \"Un groupe de personnes se tenant devant un igloo.\"\n",
    "\n",
    "tgt_language = translate(transformer, src_language, fr_vocab, en_vocab,\n",
    "                         fr_tokenizer, BOS_IDX, EOS_IDX, device)\n",
    "\n",
    "print(f\"Translated: `{tgt_language}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Attention is all you need papaer. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "2. Language Translation With Transformer Tutorial. https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv5 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
