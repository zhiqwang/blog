{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANGUAGE TRANSLATION WITH TRANSFORMER\n",
    "\n",
    "This tutorial shows, how to train a translation model from scratch using Transformer. We will be using Multi30k dataset to train a French to English translation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "  <img src=\"https://pic4.zhimg.com/v2-1719966a223d98ad48f98c2e4d71add7_r.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext has utilities for creating datasets that can be easily iterated through for the purposes of creating a language translation model. In this example, we show how to tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor.\n",
    "\n",
    "To run this tutorial, first install spacy using pip or conda. Next, download the raw data for the English and French Spacy tokenizers from https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f902cee7d80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import extract_archive\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_helper import Seq2SeqTransformer, train_epoch, evaluate, translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data-bin')\n",
    "\n",
    "train_urls = ('train.fr', 'train.en')\n",
    "val_urls = ('val.fr', 'val.en')\n",
    "test_urls = ('test_2016_flickr.fr', 'test_2016_flickr.en')\n",
    "\n",
    "train_filepaths = [data_path / url for url in train_urls]\n",
    "val_filepaths = [data_path / url for url in val_urls]\n",
    "test_filepaths = [data_path / url for url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab = build_vocab(train_filepaths[0], fr_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "    raw_fr_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = []\n",
    "    for (raw_fr, raw_en) in zip(raw_fr_iter, raw_en_iter):\n",
    "        fr_tensor_ = torch.tensor([fr_vocab[token] for token in fr_tokenizer(raw_fr.rstrip(\"\\n\"))],\n",
    "                                  dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"\\n\"))],\n",
    "                                  dtype=torch.long)\n",
    "        data.append((fr_tensor_, en_tensor_))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = fr_vocab['<pad>']\n",
    "BOS_IDX = fr_vocab['<bos>']\n",
    "EOS_IDX = fr_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last torch specific feature we’ll use is the DataLoader, which is easy to use since it takes the data as its first argument. Specifically, as the docs say: DataLoader combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.\n",
    "\n",
    "Please pay attention to collate_fn (optional) that merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    fr_batch, en_batch = [], []\n",
    "    for (fr_item, en_item) in data_batch:\n",
    "        fr_batch.append(\n",
    "            torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        en_batch.append(\n",
    "            torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return fr_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer is a Seq2Seq model introduced in [“Attention is all you need”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper for solving machine translation task. Transformer model consists of an encoder and decoder block each containing fixed number of layers.\n",
    "\n",
    "Encoder processes the input sequence by propogating it, through a series of Multi-head Attention and Feed forward network layers. The output from the Encoder referred to as `memory`, is fed to the decoder along with target tensors. Encoder and decoder are trained in an end-to-end fashion using teacher forcing technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters and instantiate model\n",
    "SRC_VOCAB_SIZE = len(fr_vocab)\n",
    "TGT_VOCAB_SIZE = len(en_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    NHEAD,\n",
    "    EMB_SIZE,\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    FFN_HID_DIM,\n",
    ")\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Train loss: 5.298, Val loss: 4.005, Epoch time = 26.077s\n",
      "Epoch:  2, Train loss: 3.585, Val loss: 3.051, Epoch time = 26.319s\n",
      "Epoch:  3, Train loss: 2.852, Val loss: 2.550, Epoch time = 26.021s\n",
      "Epoch:  4, Train loss: 2.411, Val loss: 2.271, Epoch time = 26.033s\n",
      "Epoch:  5, Train loss: 2.106, Val loss: 2.066, Epoch time = 26.306s\n",
      "Epoch:  6, Train loss: 1.877, Val loss: 1.928, Epoch time = 25.493s\n",
      "Epoch:  7, Train loss: 1.697, Val loss: 1.834, Epoch time = 24.394s\n",
      "Epoch:  8, Train loss: 1.549, Val loss: 1.742, Epoch time = 24.415s\n",
      "Epoch:  9, Train loss: 1.423, Val loss: 1.679, Epoch time = 24.200s\n",
      "Epoch: 10, Train loss: 1.318, Val loss: 1.634, Epoch time = 24.333s\n",
      "Epoch: 11, Train loss: 1.226, Val loss: 1.582, Epoch time = 24.226s\n",
      "Epoch: 12, Train loss: 1.143, Val loss: 1.571, Epoch time = 24.316s\n",
      "Epoch: 13, Train loss: 1.070, Val loss: 1.535, Epoch time = 24.195s\n",
      "Epoch: 14, Train loss: 1.000, Val loss: 1.502, Epoch time = 24.398s\n",
      "Epoch: 15, Train loss: 0.940, Val loss: 1.497, Epoch time = 24.248s\n",
      "Epoch: 16, Train loss: 0.885, Val loss: 1.487, Epoch time = 24.494s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer, PAD_IDX, device)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter, PAD_IDX, device)\n",
    "    print((f\"Epoch: {epoch:2d}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"\n",
    "           f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following results during model training.\n",
    "\n",
    "```R\n",
    "Epoch:  1, Train loss: 5.298, Val loss: 4.005, Epoch time = 24.849s\n",
    "Epoch:  2, Train loss: 3.585, Val loss: 3.051, Epoch time = 26.374s\n",
    "Epoch:  3, Train loss: 2.852, Val loss: 2.550, Epoch time = 24.791s\n",
    "Epoch:  4, Train loss: 2.411, Val loss: 2.271, Epoch time = 24.833s\n",
    "Epoch:  5, Train loss: 2.106, Val loss: 2.066, Epoch time = 25.058s\n",
    "Epoch:  6, Train loss: 1.877, Val loss: 1.928, Epoch time = 24.756s\n",
    "Epoch:  7, Train loss: 1.697, Val loss: 1.834, Epoch time = 25.069s\n",
    "Epoch:  8, Train loss: 1.549, Val loss: 1.742, Epoch time = 25.382s\n",
    "Epoch:  9, Train loss: 1.423, Val loss: 1.679, Epoch time = 24.702s\n",
    "Epoch: 10, Train loss: 1.318, Val loss: 1.634, Epoch time = 24.820s\n",
    "Epoch: 11, Train loss: 1.226, Val loss: 1.582, Epoch time = 24.807s\n",
    "Epoch: 12, Train loss: 1.143, Val loss: 1.571, Epoch time = 24.967s\n",
    "Epoch: 13, Train loss: 1.070, Val loss: 1.535, Epoch time = 24.855s\n",
    "Epoch: 14, Train loss: 1.000, Val loss: 1.502, Epoch time = 25.069s\n",
    "Epoch: 15, Train loss: 0.940, Val loss: 1.497, Epoch time = 24.914s\n",
    "Epoch: 16, Train loss: 0.885, Val loss: 1.487, Epoch time = 25.155s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: `A group of people standing in front of an igloo .`.\n"
     ]
    }
   ],
   "source": [
    "src_language = \"Un groupe de personnes se tenant devant un igloo .\"\n",
    "\n",
    "tgt_language = translate(transformer, src_language, fr_vocab, en_vocab,\n",
    "                         fr_tokenizer, BOS_IDX, EOS_IDX, device)\n",
    "\n",
    "print(f\"Translated: `{tgt_language}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Attention is all you need papaer. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "2. Language Translation With Transformer Tutorial. https://pytorch.org/tutorials/beginner/translation_transformer.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv5 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
